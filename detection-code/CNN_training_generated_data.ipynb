{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet based 1-D CNN segmentation training\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook demonstrates the process of training a 1-D CNN segmentation model using a U-Net architecture. In it we perform data preprocessing, model definition, training, validation, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import segmentation_models_pytorch as smp\n",
    "import random\n",
    "random.seed(10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#The notebook has not been tested on GPUs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Unet_1D_model import build_unet\n",
    "from src.data_augmentation import augment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split into train, validation and testing sets with a split of 70%,20%,10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(\"data\")\n",
    "with open(data_path / \"labeled_data_fileNames.txt\") as f:\n",
    "    _list_of_files = f.readlines()\n",
    "list_of_files = [x.replace('\\n','') for x in _list_of_files]\n",
    "print(list_of_files)\n",
    "print(len(list_of_files))\n",
    "\n",
    "all_data=[]\n",
    "all_labels=[]\n",
    "\n",
    "for n,file in enumerate(list_of_files):\n",
    "    data,labels=np.loadtxt(data_path / file,delimiter=',')\n",
    "\n",
    "    rsc=RobustScaler(quantile_range=(30.0, 70.0),unit_variance=True)\n",
    "    data=data.reshape(-1,1)\n",
    "    \n",
    "    data=rsc.fit_transform(data)\n",
    "\n",
    "    all_data.append(data)\n",
    "    all_labels.append(labels)\n",
    "X=np.array(all_data)\n",
    "y=np.array(all_labels)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.22, random_state=101) # 0.22 x 0.9 = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing\n",
    "\n",
    "To be able to train a CNN on the input data, it needs to be split into windows of equal size. The window size was set to 2000, with a stride of 1000 so that if a peak is split in half by the window another window would have the full peak in the middle. The last window consists of the last data points of the series as well as looped data from the start so a 2000 size window could be created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "window_size=2000\n",
    "stride=window_size//2\n",
    "# print(stride)\n",
    "\n",
    "def create_windows(data, window_size, stride):\n",
    "    missing_vals = window_size - (len(data) % window_size)\n",
    "    data_looped = np.append(data, data[:missing_vals + window_size])\n",
    "    return np.array([data_looped[i:i + window_size] for i in range(0, len(data) + 1, stride)])\n",
    "\n",
    "\n",
    "def windowing(data,labels):\n",
    "    data_windows = []\n",
    "    label_windows = []\n",
    "    for data, label in zip(data, labels):\n",
    "        data_windows.append(create_windows(data, window_size, stride))\n",
    "        label_windows.append(create_windows(label, window_size, stride))\n",
    "    return np.concatenate(data_windows,dtype=np.float32), np.concatenate(label_windows)\n",
    "\n",
    "X_train_windows, y_train_windows = windowing(X_train, y_train)\n",
    "X_val_windows, y_val_windows = windowing(X_val, y_val)\n",
    "X_test_windows, y_test_windows = windowing(X_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Here we prepare the windowed data for training in a pytorch model. We onehot encode the target values. Then we put the training, validation and test datasets into a TimeSeriesDataSet structure. One can also add augmentation to the training set, in theory this will help with generalization but it didn't give much effect in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1\n",
    "\n",
    "X_train_input = X_train_windows.reshape((len(X_train_windows), n_features, window_size))\n",
    "X_val_input = X_val_windows.reshape((len(X_val_windows), n_features, window_size))\n",
    "X_test_input = X_test_windows.reshape((len(X_test_windows), n_features, window_size))\n",
    "\n",
    "train_target_onehot=torch.nn.functional.one_hot(torch.tensor(y_train_windows).long(),num_classes=2)\n",
    "train_target_onehot=train_target_onehot.transpose(1,2)\n",
    "val_target_onehot=torch.nn.functional.one_hot(torch.tensor(y_val_windows).long(),num_classes=2)\n",
    "val_target_onehot=val_target_onehot.transpose(1,2)\n",
    "test_Y_windows_onehot=torch.nn.functional.one_hot(torch.tensor(y_test_windows).long(),num_classes=2)\n",
    "test_Y_windows_onehot=test_Y_windows_onehot.transpose(1,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "batch_size = 16\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, targets, augment=False):\n",
    "        print(data.shape)\n",
    "        if augment:\n",
    "            data = augment_data(data)\n",
    "        print(data.shape)\n",
    "        self.data = torch.from_numpy(data.astype('float32').copy()).clone().to(device)\n",
    "        self.targets = targets.clone().to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.targets[idx]\n",
    "        \n",
    "        return x,y\n",
    "\n",
    "# Create original dataset\n",
    "train_dataset = TimeSeriesDataset(X_train_input, train_target_onehot, augment=False)\n",
    "\n",
    "# Create augmented dataset\n",
    "train_dataset_augmented = TimeSeriesDataset(X_train_input, train_target_onehot, augment=True)\n",
    "\n",
    "# Combine original and augmented datasets\n",
    "train_dataset = ConcatDataset([train_dataset, train_dataset_augmented])\n",
    "# Create DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.tensor(X_val_input).to(device), val_target_onehot)\n",
    "test_dataset=torch.utils.data.TensorDataset(torch.tensor(X_test_input).to(device), test_Y_windows_onehot)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model used is a 1-D version of the UNet segmentation model. Below is code used to print a summary of the model and it's parameters. The proper model used during training is defined lower down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "model = build_unet(input_channels=1, num_classes=2,layer_n=24,conv_kernel_size=3,scaling_kernel_size=2)\n",
    "print(summary(model, (1, window_size), batch_size=batch_size, device=device.type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Here we import the training loop from src/CNN_training.py and perform training with the parameters we choose. The model class is defined in src/Unet_1D_model.py. The arguments to the training loop are:\n",
    "\n",
    "- data_loaders (tuple): A tuple containing the training and validation data loaders.\n",
    "- config (dict): A dictionary containing the configuration parameters for the model and training process.\n",
    "\n",
    "The expected keys in the config which include many hyperparameters of the model:\n",
    "\n",
    "- 'num_classes' (int): Number of output classes.\n",
    "- 'layer_n' (int): Number of layers in the U-Net model.\n",
    "- 'dropout' (float): Dropout rate.\n",
    "- 'loss_fn' (str): Loss function to use ('CE','BCE', 'DL', 'DLLog').\n",
    "- 'lr' (float): Learning rate.\n",
    "- 'batch_size' (int): Batch size.\n",
    "- 'n_epochs' (int): Number of epochs.\n",
    "- 'saveString' (str): Prefix for saving model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.CNN_training import training_loop\n",
    "\n",
    "config={'layer_n':24,\n",
    "        'dropout':0.2,\n",
    "        'batch_size':batch_size,\n",
    "        'n_epochs':30,\n",
    "        'lr':0.0003,\n",
    "        'saveString':'generatedData_RobustScaled3070_1DUnet',\n",
    "        'loss_fn':'BCE',\n",
    "        'save_best_training_model':False,\n",
    "        'num_classes':2,}\n",
    "\n",
    "model,loss_values, version_name = training_loop((train_loader,valid_loader), config)\n",
    "\n",
    "# loss_values=np.array(loss_values)\n",
    "plt.figure()\n",
    "plt.plot(loss_values[:,0],loss_values[:,1],label='train_loss')\n",
    "plt.plot(loss_values[:,0],loss_values[:,5],label='val_loss')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/\"+version_name+\"_loss.png\")\n",
    "# plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_values[:,0],loss_values[:,2],label='train_f1')\n",
    "plt.plot(loss_values[:,0],loss_values[:,6],label='val_f1')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/\"+version_name+\"_f1.png\")\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_values[:,0],loss_values[:,1],label='train_loss')\n",
    "plt.plot(loss_values[:,0],loss_values[:,5],label='val_loss')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"figures/\"+version_name+\"_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_values[:,0],loss_values[:,2],label='train_f1')\n",
    "plt.plot(loss_values[:,0],loss_values[:,6],label='val_f1')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"figures/\"+version_name+\"_f1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Here we test the performance of the model on the test dataset. The model can be loaded from the checkpoint. We can also show some plots of the windows to see the performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version name can be defined manually or taken from the training loop\n",
    "# version_name=\"T015_event2_event9_widelabeled_RobustScaled3070_noBN_build_unet_layer24_dropout0.2_conv3_scaling2_lossBCE_lr0.0005_bs16_epochs50\"\n",
    "\n",
    "model = torch.jit.load(\"Models/\" + version_name + \"_validationcheckpoint_Models.pt\")\n",
    "\n",
    "# model = torch.jit.load('fullModel/T015_event2_event9_widelabeled_RobustScaled3070_noBN_build_unet_layer20_dropout0.0_conv3_scaling2_lossBCE_lr0.001_bs32_validationcheckpoint_fullmodel.pt')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_shape = (len(test_loader.dataset),) + test_loader.dataset[0][0].shape\n",
    "test_preds = np.zeros((int(test_shape[0] * test_shape[2])))\n",
    "test_targets = np.zeros((int(test_shape[0] * test_shape[2])))\n",
    "import sys\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "\n",
    "total = test_targets.size\n",
    "\n",
    "test_shape = (len(test_loader.dataset),) + test_loader.dataset[0][0].shape\n",
    "\n",
    "for i, (x_batch, y_batch) in enumerate(test_loader):\n",
    "    y_pred = model(x_batch).detach()\n",
    "\n",
    "    pred = F.softmax(y_pred, 1).detach().cpu().numpy().argmax(axis=1)\n",
    "    y_batch_pred = y_batch.detach().cpu().numpy().argmax(axis=1)\n",
    "\n",
    "    test_preds[\n",
    "        i * batch_size * test_shape[2] : (i + 1) * batch_size * test_shape[2]\n",
    "    ] = pred.reshape((-1))\n",
    "    test_targets[\n",
    "        i * batch_size * test_shape[2] : (i + 1) * batch_size * test_shape[2]\n",
    "    ] = y_batch_pred.reshape((-1))\n",
    "\n",
    "    # Plot some windows to see the predictions\n",
    "    if i < 5:\n",
    "        j = 4\n",
    "        print(j * 1000)\n",
    "        x_batch0 = x_batch[j].flatten().numpy()\n",
    "\n",
    "        y_batch_pred0 = y_batch_pred[j]\n",
    "        pred0 = pred[j]\n",
    "        mean = np.mean(x_batch0)\n",
    "        std = np.std(x_batch0)\n",
    "        y_batch_plot = y_batch_pred0 * std / 3.1 + mean\n",
    "        y_pred_plot = pred0 * std / 2.9 + mean\n",
    "\n",
    "        data_smooth = (\n",
    "            pd.Series(x_batch0)\n",
    "            .rolling(80, center=True, min_periods=1)\n",
    "            .mean()\n",
    "            .to_numpy()\n",
    "        )\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.plot(data_smooth, label=\"data smooth\")\n",
    "        plt.plot(y_batch_plot, label=\"label true\")\n",
    "        plt.plot(y_pred_plot, label=\"label pred\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    del y_pred, x_batch, y_batch, pred\n",
    "\n",
    "print(f\"Test Predictions: 0: {sum(test_preds==0)}, 1: {sum(test_preds==1)}\")\n",
    "print(f\"Test Targets: 0: {sum(test_targets==0)}, 1: {sum(test_targets==1)}\")\n",
    "\n",
    "print(\"TEST_SCORE (F1): \", f1_score(test_targets, test_preds, average=\"macro\"))\n",
    "correct = (test_preds == test_targets).sum().item()\n",
    "\n",
    "print(\n",
    "    f\"Accuracy of the network on the {test_shape[0]} train ranges: {100 * correct // total} %\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw_detect_ml",
   "language": "python",
   "name": "sw_detect_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
